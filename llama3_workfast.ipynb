{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db571de-b9c7-44b6-9216-2d21f01f349f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T19:26:04.937575Z",
     "iopub.status.busy": "2024-06-08T19:26:04.937575Z",
     "iopub.status.idle": "2024-06-08T19:26:04.941385Z",
     "shell.execute_reply": "2024-06-08T19:26:04.941385Z",
     "shell.execute_reply.started": "2024-06-08T19:26:04.937575Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2d21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fbc666",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe8e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp.llama_cpp import _load_shared_library\n",
    "\n",
    "def is_gpu_available_v3() -> bool:\n",
    "    lib = _load_shared_library('llama')\n",
    "    return bool(lib.llama_supports_gpu_offload())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffa2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_gpu_available_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82144cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5510943",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b8fa5e-dd0b-4fce-950c-9e82367dd1dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T18:37:32.465942Z",
     "iopub.status.busy": "2024-06-08T18:37:32.465942Z",
     "iopub.status.idle": "2024-06-08T18:37:33.207733Z",
     "shell.execute_reply": "2024-06-08T18:37:33.207733Z",
     "shell.execute_reply.started": "2024-06-08T18:37:32.465942Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "#import pdfplumber\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_cpp import Llama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms  import LlamaCpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d13ad-0f28-408d-85ce-824122137ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:44:58.496063Z",
     "iopub.status.busy": "2024-06-05T04:44:58.496063Z",
     "iopub.status.idle": "2024-06-05T04:44:58.638167Z",
     "shell.execute_reply": "2024-06-05T04:44:58.638167Z",
     "shell.execute_reply.started": "2024-06-05T04:44:58.496063Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#first version\n",
    "#now we load in the pdfs\n",
    "document = []\n",
    "for file in os.listdir(\"docs\"):\n",
    "    print(file)\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = \".\\\\docs\\\\\"+file\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        document.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb321349-fe43-41ff-b0d1-8ab934ab087a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:44:58.638167Z",
     "iopub.status.busy": "2024-06-05T04:44:58.638167Z",
     "iopub.status.idle": "2024-06-05T04:44:58.640429Z",
     "shell.execute_reply": "2024-06-05T04:44:58.640429Z",
     "shell.execute_reply.started": "2024-06-05T04:44:58.638167Z"
    }
   },
   "outputs": [],
   "source": [
    "# #second version\n",
    "# def extract_text_from_pdf(file_path):\n",
    "#     with pdf.plumber.open(file_path) as pdf:\n",
    "#         text = \"\"\n",
    "#         for page in pdf.pages:\n",
    "#             text += page.extract_text()\n",
    "        \n",
    "#     return text\n",
    "\n",
    "# def extract_many_files(folder_name):\n",
    "#     for file in os.listdir(folder_name):\n",
    "#         if file.endswith(\".pdf\"):\n",
    "#             pdf_path = \".\\\\\" + folder_name + \"\\\\\"+file\n",
    "#             text = extract_text_from_pdf(pdf_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc79c9d-c326-44cc-b7c8-cdba7acd11bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:44:58.640429Z",
     "iopub.status.busy": "2024-06-05T04:44:58.640429Z",
     "iopub.status.idle": "2024-06-05T04:44:58.647653Z",
     "shell.execute_reply": "2024-06-05T04:44:58.647653Z",
     "shell.execute_reply.started": "2024-06-05T04:44:58.640429Z"
    }
   },
   "outputs": [],
   "source": [
    "#tried 500, 0\n",
    "#tried 1000, 200\n",
    "document_splitta=CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "document_chunks=document_splitta.split_documents(document)\n",
    "len(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ddd866-ecd9-4429-bc66-2e62fbb1bdf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:44:58.648657Z",
     "iopub.status.busy": "2024-06-05T04:44:58.647653Z",
     "iopub.status.idle": "2024-06-05T04:45:02.057093Z",
     "shell.execute_reply": "2024-06-05T04:45:02.057093Z",
     "shell.execute_reply.started": "2024-06-05T04:44:58.648657Z"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5cb0d4-2e7d-4ca9-8587-c2634f2a855f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:02.058096Z",
     "iopub.status.busy": "2024-06-05T04:45:02.058096Z",
     "iopub.status.idle": "2024-06-05T04:45:02.379920Z",
     "shell.execute_reply": "2024-06-05T04:45:02.379920Z",
     "shell.execute_reply.started": "2024-06-05T04:45:02.058096Z"
    }
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(document_chunks, embedding=embeddings, persist_directory='.\\\\data')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722a650c-9774-4053-8bb9-4d96ba3459b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:02.380923Z",
     "iopub.status.busy": "2024-06-05T04:45:02.380923Z",
     "iopub.status.idle": "2024-06-05T04:45:02.385010Z",
     "shell.execute_reply": "2024-06-05T04:45:02.385010Z",
     "shell.execute_reply.started": "2024-06-05T04:45:02.380923Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"QuantFactory/Meta-Llama-3-8B-GGUF\"\n",
    "model_file = \"Meta-Llama-3-8B.Q8_0.gguf\"\n",
    "model_path = \".\\\\llms\\\\Meta-Llama-3-8B-Instruct.Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c59e8f-8ff4-40fe-b1ac-58476cb8625b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:02.386013Z",
     "iopub.status.busy": "2024-06-05T04:45:02.386013Z",
     "iopub.status.idle": "2024-06-05T04:45:02.392572Z",
     "shell.execute_reply": "2024-06-05T04:45:02.392572Z",
     "shell.execute_reply.started": "2024-06-05T04:45:02.386013Z"
    }
   },
   "outputs": [],
   "source": [
    "# llm = Llama(\n",
    "#     model_path=model_path,\n",
    "#     n_ctx=4096,\n",
    "#     n_gpu_layers=80,\n",
    "#     n_batch=521\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1580f53e-b92b-4fe0-9ff4-e8f73c41b768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:02.392572Z",
     "iopub.status.busy": "2024-06-05T04:45:02.392572Z",
     "iopub.status.idle": "2024-06-05T04:45:05.234003Z",
     "shell.execute_reply": "2024-06-05T04:45:05.234003Z",
     "shell.execute_reply.started": "2024-06-05T04:45:02.392572Z"
    }
   },
   "outputs": [],
   "source": [
    "llm = LlamaCpp(model_path=model_path,temperature=0.3,n_gpu_layers= 80,\n",
    "        vocab_only = False,\n",
    "        use_mmap = True,\n",
    "        use_mlock = False,\n",
    "        # Context Params                                                                                                                                                                                                                                                          \n",
    "        seed=  0xFFFFFFFF,\n",
    "        n_ctx = 4096,\n",
    "        n_batch = 521,\n",
    "        n_threads= None,\n",
    "        rope_freq_base = 0.0,\n",
    "        rope_freq_scale = 0.0,\n",
    "        f16_kv = True,\n",
    "        logits_all = False,\n",
    "        embedding = False,\n",
    "        # Sampling Params                                                                                                                                                                                                                                                         \n",
    "        last_n_tokens_size = 64,\n",
    "        # LoRA Params                                                                                                                                                                                                                                                             \n",
    "        lora_base = None,\n",
    "        lora_scale = 1.0,\n",
    "        lora_path = None,\n",
    "        # Backend Params                                                                                                                                                                                                                                                          \n",
    "        numa= False,\n",
    "        # Chat Format Params                                                                                                                                                                                                                                                      \n",
    "        # Misc                                                                                                                                                                                                                                                                    \n",
    "        verbose = True,\n",
    "        max_tokens=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8926048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_it(question):\n",
    "    \"\"\"Given a question return an answer using the rag pipeline\"\"\"\n",
    "    question = question[0:-1] + \"using only the documents given and no prior knowledge\"\n",
    "    docs = vectordb.similarity_search(question)\n",
    "    rag_pipeline = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",retriever=vectordb.as_retriever())\n",
    "    print(\"Here are the docs that were picked up: \")\n",
    "    for doc in docs:\n",
    "        print(doc)\n",
    "    return (rag_pipeline(question))['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9432c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Given these documents, give me the difference between the iPhone 11 Pro Max and the iPhone 13 Pro Max's features.\"\n",
    "docs = vectordb.similarity_search(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b27306",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\",retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b6a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_pipeline(\"Given these documents, give me the difference between the iPhone 11 Pro Max and the iPhone 13 Pro Max's features using only these documents and not prior knowledge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the dimensions of iPhone 11 Pro Max and iPhone SE?\"\n",
    "answer2 =  rag_it(question)\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the display sizes of the iPhone 11 Pro Max and iPhone SE?\"\n",
    "answer =  rag_it(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff255c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the display of iPhone 11 Pro Max?\"\n",
    "answer =  rag_it(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the display sizes of the iPhone SE and iPhone 13 Pro Max?\"\n",
    "answer =  rag_it(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf5736-b64f-4abf-9a7f-47a6ac5758c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:05.234003Z",
     "iopub.status.busy": "2024-06-05T04:45:05.234003Z",
     "iopub.status.idle": "2024-06-05T04:45:05.236618Z",
     "shell.execute_reply": "2024-06-05T04:45:05.236618Z",
     "shell.execute_reply.started": "2024-06-05T04:45:05.234003Z"
    }
   },
   "outputs": [],
   "source": [
    "# memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "# prompt_template = PromptTemplate.from_template('Use these documents to answer questions that I ask about them')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc89b1-47cc-4182-89b2-509e2e0202ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:05.236618Z",
     "iopub.status.busy": "2024-06-05T04:45:05.236618Z",
     "iopub.status.idle": "2024-06-05T04:45:05.248056Z",
     "shell.execute_reply": "2024-06-05T04:45:05.248056Z",
     "shell.execute_reply.started": "2024-06-05T04:45:05.236618Z"
    }
   },
   "outputs": [],
   "source": [
    "# llama_model = LLMChain(llm=llm, prompt=prompt_template)\n",
    "# pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectordb.as_retriever(search_kwargs={'k':6}),verbose=False,memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040c979-8703-4173-9625-382d3ad01272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:05.249061Z",
     "iopub.status.busy": "2024-06-05T04:45:05.248056Z",
     "iopub.status.idle": "2024-06-05T04:45:05.952022Z",
     "shell.execute_reply": "2024-06-05T04:45:05.952022Z",
     "shell.execute_reply.started": "2024-06-05T04:45:05.249061Z"
    }
   },
   "outputs": [],
   "source": [
    "# query=\"What is the main topic of the documents?\"\n",
    "# response=pdf_qa({\"question\": query})\n",
    "# print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b1b68-3157-460f-8443-878d8016de78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:05.952022Z",
     "iopub.status.busy": "2024-06-05T04:45:05.952022Z",
     "iopub.status.idle": "2024-06-05T04:45:07.750183Z",
     "shell.execute_reply": "2024-06-05T04:45:07.750183Z",
     "shell.execute_reply.started": "2024-06-05T04:45:05.952022Z"
    }
   },
   "outputs": [],
   "source": [
    "# query=\"Given these documents, give me the difference between the iPhone 11 Pro max and the iPhone 13 Pro Max features.\"\n",
    "# response=pdf_qa({\"question\": query})\n",
    "# print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc60b60-b521-4ae4-ace1-b712258419f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
