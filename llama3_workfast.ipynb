{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9db571de-b9c7-44b6-9216-2d21f01f349f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T19:26:04.937575Z",
     "iopub.status.busy": "2024-06-08T19:26:04.937575Z",
     "iopub.status.idle": "2024-06-08T19:26:04.941385Z",
     "shell.execute_reply": "2024-06-08T19:26:04.941385Z",
     "shell.execute_reply.started": "2024-06-08T19:26:04.937575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:42:21) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2d21b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fbc666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: llama_cpp_python\n",
      "Version: 0.2.77\n",
      "Summary: Python bindings for the llama.cpp library\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Andrei Betlen <abetlen@gmail.com>\n",
      "License: MIT\n",
      "Location: c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\n",
      "Requires: diskcache, jinja2, numpy, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show llama-cpp-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe8e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp.llama_cpp import _load_shared_library\n",
    "\n",
    "def is_gpu_available_v3() -> bool:\n",
    "    lib = _load_shared_library('llama')\n",
    "    return bool(lib.llama_supports_gpu_offload())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffa2db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_gpu_available_v3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82144cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5510943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2b8fa5e-dd0b-4fce-950c-9e82367dd1dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T18:37:32.465942Z",
     "iopub.status.busy": "2024-06-08T18:37:32.465942Z",
     "iopub.status.idle": "2024-06-08T18:37:33.207733Z",
     "shell.execute_reply": "2024-06-08T18:37:33.207733Z",
     "shell.execute_reply.started": "2024-06-08T18:37:32.465942Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.docstore.document import Document\n",
    "#import pdfplumber\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_cpp import Llama\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms  import LlamaCpp\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a03d13ad-0f28-408d-85ce-824122137ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:44:58.496063Z",
     "iopub.status.busy": "2024-06-05T04:44:58.496063Z",
     "iopub.status.idle": "2024-06-05T04:44:58.638167Z",
     "shell.execute_reply": "2024-06-05T04:44:58.638167Z",
     "shell.execute_reply.started": "2024-06-05T04:44:58.496063Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip11pmaxspecs.pdf\n",
      "iphone13pm.pdf\n",
      "iphoneSEspecs.pdf\n"
     ]
    }
   ],
   "source": [
    "#first version\n",
    "#now we load in the pdfs\n",
    "document = []\n",
    "for file in os.listdir(\"docs\"):\n",
    "    print(file)\n",
    "    if file.endswith(\".pdf\"):\n",
    "        pdf_path = \".\\\\docs\\\\\"+file\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        document.extend(loader.load())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb321349-fe43-41ff-b0d1-8ab934ab087a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:44:58.638167Z",
     "iopub.status.busy": "2024-06-05T04:44:58.638167Z",
     "iopub.status.idle": "2024-06-05T04:44:58.640429Z",
     "shell.execute_reply": "2024-06-05T04:44:58.640429Z",
     "shell.execute_reply.started": "2024-06-05T04:44:58.638167Z"
    }
   },
   "outputs": [],
   "source": [
    "# #second version\n",
    "# def extract_text_from_pdf(file_path):\n",
    "#     with pdf.plumber.open(file_path) as pdf:\n",
    "#         text = \"\"\n",
    "#         for page in pdf.pages:\n",
    "#             text += page.extract_text()\n",
    "        \n",
    "#     return text\n",
    "\n",
    "# def extract_many_files(folder_name):\n",
    "#     for file in os.listdir(folder_name):\n",
    "#         if file.endswith(\".pdf\"):\n",
    "#             pdf_path = \".\\\\\" + folder_name + \"\\\\\"+file\n",
    "#             text = extract_text_from_pdf(pdf_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc79c9d-c326-44cc-b7c8-cdba7acd11bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:44:58.640429Z",
     "iopub.status.busy": "2024-06-05T04:44:58.640429Z",
     "iopub.status.idle": "2024-06-05T04:44:58.647653Z",
     "shell.execute_reply": "2024-06-05T04:44:58.647653Z",
     "shell.execute_reply.started": "2024-06-05T04:44:58.640429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tried 500, 0\n",
    "#tried 1000, 200\n",
    "document_splitta=CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "document_chunks=document_splitta.split_documents(document)\n",
    "len(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66ddd866-ecd9-4429-bc66-2e62fbb1bdf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:44:58.648657Z",
     "iopub.status.busy": "2024-06-05T04:44:58.647653Z",
     "iopub.status.idle": "2024-06-05T04:45:02.057093Z",
     "shell.execute_reply": "2024-06-05T04:45:02.057093Z",
     "shell.execute_reply.started": "2024-06-05T04:44:58.648657Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad5cb0d4-2e7d-4ca9-8587-c2634f2a855f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:02.058096Z",
     "iopub.status.busy": "2024-06-05T04:45:02.058096Z",
     "iopub.status.idle": "2024-06-05T04:45:02.379920Z",
     "shell.execute_reply": "2024-06-05T04:45:02.379920Z",
     "shell.execute_reply.started": "2024-06-05T04:45:02.058096Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "vectordb = Chroma.from_documents(document_chunks, embedding=embeddings, persist_directory='.\\\\data')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "722a650c-9774-4053-8bb9-4d96ba3459b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:02.380923Z",
     "iopub.status.busy": "2024-06-05T04:45:02.380923Z",
     "iopub.status.idle": "2024-06-05T04:45:02.385010Z",
     "shell.execute_reply": "2024-06-05T04:45:02.385010Z",
     "shell.execute_reply.started": "2024-06-05T04:45:02.380923Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"QuantFactory/Meta-Llama-3-8B-GGUF\"\n",
    "model_file = \"Meta-Llama-3-8B.Q8_0.gguf\"\n",
    "model_path = \".\\\\llms\\\\Meta-Llama-3-8B-Instruct.Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95c59e8f-8ff4-40fe-b1ac-58476cb8625b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:02.386013Z",
     "iopub.status.busy": "2024-06-05T04:45:02.386013Z",
     "iopub.status.idle": "2024-06-05T04:45:02.392572Z",
     "shell.execute_reply": "2024-06-05T04:45:02.392572Z",
     "shell.execute_reply.started": "2024-06-05T04:45:02.386013Z"
    }
   },
   "outputs": [],
   "source": [
    "# llm = Llama(\n",
    "#     model_path=model_path,\n",
    "#     n_ctx=4096,\n",
    "#     n_gpu_layers=80,\n",
    "#     n_batch=521\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1580f53e-b92b-4fe0-9ff4-e8f73c41b768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:02.392572Z",
     "iopub.status.busy": "2024-06-05T04:45:02.392572Z",
     "iopub.status.idle": "2024-06-05T04:45:05.234003Z",
     "shell.execute_reply": "2024-06-05T04:45:05.234003Z",
     "shell.execute_reply.started": "2024-06-05T04:45:02.392572Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! embedding is not default parameter.\n",
      "                embedding was transferred to model_kwargs.\n",
      "                Please confirm that embedding is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! lora_scale is not default parameter.\n",
      "                lora_scale was transferred to model_kwargs.\n",
      "                Please confirm that lora_scale is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! numa is not default parameter.\n",
      "                numa was transferred to model_kwargs.\n",
      "                Please confirm that numa is what you intended.\n",
      "  warnings.warn(\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from .\\llms\\Meta-Llama-3-8B-Instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = .\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q8_0:  226 tensors\n",
      "llm_load_vocab: missing pre-tokenizer type, using: 'default'\n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab: GENERATION QUALITY WILL BE DEGRADED!        \n",
      "llm_load_vocab: CONSIDER REGENERATING THE MODEL             \n",
      "llm_load_vocab: ************************************        \n",
      "llm_load_vocab:                                             \n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 7.95 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = .\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   yes\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   532.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  7605.33 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 521\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.name': '.', 'general.architecture': 'llama', 'llama.block_count': '32', 'llama.vocab_size': '128256', 'llama.context_length': '8192', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '128009', 'general.file_type': '7', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.model': 'gpt2', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '128000', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(model_path=model_path,temperature=0.3,n_gpu_layers= 80,\n",
    "        vocab_only = False,\n",
    "        use_mmap = True,\n",
    "        use_mlock = False,\n",
    "        # Context Params                                                                                                                                                                                                                                                          \n",
    "        seed=  0xFFFFFFFF,\n",
    "        n_ctx = 4096,\n",
    "        n_batch = 521,\n",
    "        n_threads= None,\n",
    "        rope_freq_base = 0.0,\n",
    "        rope_freq_scale = 0.0,\n",
    "        f16_kv = True,\n",
    "        logits_all = False,\n",
    "        embedding = False,\n",
    "        # Sampling Params                                                                                                                                                                                                                                                         \n",
    "        last_n_tokens_size = 64,\n",
    "        # LoRA Params                                                                                                                                                                                                                                                             \n",
    "        lora_base = None,\n",
    "        lora_scale = 1.0,\n",
    "        lora_path = None,\n",
    "        # Backend Params                                                                                                                                                                                                                                                          \n",
    "        numa= False,\n",
    "        # Chat Format Params                                                                                                                                                                                                                                                      \n",
    "        # Misc                                                                                                                                                                                                                                                                    \n",
    "        verbose = True,\n",
    "        max_tokens=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8926048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_it(question):\n",
    "    \"\"\"Given a question return an answer using the rag pipeline\"\"\"\n",
    "    question = question[0:-1] + \"using only the documents given and no prior knowledge\"\n",
    "    docs = vectordb.similarity_search(question)\n",
    "    rag_pipeline = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",retriever=vectordb.as_retriever())\n",
    "    print(\"Here are the docs that were picked up: \")\n",
    "    for doc in docs:\n",
    "        print(doc)\n",
    "    return (rag_pipeline(question))['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9432c330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='iPhone 13 Pro  Max  Specifications  \\nNETWORK  Technology  GSM  / CDMA  / HSPA  / EVDO  / LTE / 5G    \\nLAUNCH  Announced  2021, September 14  \\nStatus  Release 2021, September 24  \\nBODY  Dimensions  160.8 x 78.1 x 7.7 mm (6.33 x 3.07 x 0.30 in)  \\nWeight  240 g (8.47 oz)  \\nBuild  Glass front (Gorilla Glass), glass back (Gorilla Glass), stainless steel frame  \\nSIM  Single SIM (Nano -SIM and/or eSIM) or Dual SIM (Nano -SIM/eSIM, dual stand -by) \\n  IP68 dust/water resistant (up to 6m for 30 mins)  \\nApple Pay (Visa, MasterCard, AMEX certified)  \\nDISPLAY  Type  Super Retina XDR OLED, 120Hz, HDR10, Dolby Vision, 1000 nits (typ), 1200 nits \\n(peak)  \\nSize  6.7 inches, 109.8 cm2 (~87.4% screen -to-body ratio)  \\nResolution  1284 x 2778 pixels, 19.5:9 ratio (~458 ppi density)  \\nProtection  Scratch -resistant ceramic glass, oleophobic coating  \\n  Wide color gamut  \\nTrue-tone \\nPLATFORM  OS  iOS 15  \\nChipset  Apple A15 Bionic (5 nm)  \\nCPU  Hexa -core (2x3.22 GHz + 4xX.X GHz)  \\nGPU  Apple GPU (5 -core graphics)  \\nMEMORY  Card  slot  No \\nInternal  128GB 6GB RAM, 256GB 6GB RAM, 512GB 6GB RAM, 1TB  \\n  NVMe  \\nMAIN \\nCAMERA  Quad  12 MP, f/1.5, 26mm (wide), 1.9µm, dual pixel PDAF, sensor -shift OIS  \\n12 MP, f/2.8, 77mm (telephoto), PDAF, OIS, 3x optical zoom  \\n12 MP, f/1.8, 13mm, 120˚ (ultrawide), PDAF  \\nTOF 3D LiDAR scanner (depth)  \\nFeatures  Dual-LED dual -tone flash, HDR (photo/panorama)  \\nVideo  4K@24/30/60fps, 1080p@30/60/120/240fps, 10 -bit HDR, Dolby Vision HDR (up \\nto 60fps), ProRes, Cinematic mode, stereo sound rec.  \\nSELFIE \\nCAMERA  Dual  12 MP, f/2.2, 23mm (wide), 1/3.6\"  \\nSL 3D, (depth/biometrics sensor)  \\nFeatures  HDR  \\nVideo  4K@24/25/30/60fps, 1080p@30/60/120fps, gyro -EIS \\nSOUND  Loudspeaker  Yes, with stereo speakers  \\n3.5mm  jack  No \\nCOMMS  WLAN  Wi-Fi 802.11 a/b/g/n/ac/6, dual -band, hotspot  \\nBluetooth  5.0, A2DP, LE  \\nGPS  Yes, with A -GPS, GLONASS, GALILEO, BDS, QZSS  \\nNFC  Yes \\nRadio  No \\nUSB  Lightning, USB 2.0  \\nSensors  Face ID, accelerometer, gyro, proximity, compass, barometer', metadata={'page': 0, 'source': '.\\\\docs\\\\iphone13pm.pdf'}),\n",
       " Document(page_content='iPhone 13 Pro  Max  Specifications  \\nNETWORK  Technology  GSM  / CDMA  / HSPA  / EVDO  / LTE / 5G    \\nLAUNCH  Announced  2021, September 14  \\nStatus  Release 2021, September 24  \\nBODY  Dimensions  160.8 x 78.1 x 7.7 mm (6.33 x 3.07 x 0.30 in)  \\nWeight  240 g (8.47 oz)  \\nBuild  Glass front (Gorilla Glass), glass back (Gorilla Glass), stainless steel frame  \\nSIM  Single SIM (Nano -SIM and/or eSIM) or Dual SIM (Nano -SIM/eSIM, dual stand -by) \\n  IP68 dust/water resistant (up to 6m for 30 mins)  \\nApple Pay (Visa, MasterCard, AMEX certified)  \\nDISPLAY  Type  Super Retina XDR OLED, 120Hz, HDR10, Dolby Vision, 1000 nits (typ), 1200 nits \\n(peak)  \\nSize  6.7 inches, 109.8 cm2 (~87.4% screen -to-body ratio)  \\nResolution  1284 x 2778 pixels, 19.5:9 ratio (~458 ppi density)  \\nProtection  Scratch -resistant ceramic glass, oleophobic coating  \\n  Wide color gamut  \\nTrue-tone \\nPLATFORM  OS  iOS 15  \\nChipset  Apple A15 Bionic (5 nm)  \\nCPU  Hexa -core (2x3.22 GHz + 4xX.X GHz)  \\nGPU  Apple GPU (5 -core graphics)  \\nMEMORY  Card  slot  No \\nInternal  128GB 6GB RAM, 256GB 6GB RAM, 512GB 6GB RAM, 1TB  \\n  NVMe  \\nMAIN \\nCAMERA  Quad  12 MP, f/1.5, 26mm (wide), 1.9µm, dual pixel PDAF, sensor -shift OIS  \\n12 MP, f/2.8, 77mm (telephoto), PDAF, OIS, 3x optical zoom  \\n12 MP, f/1.8, 13mm, 120˚ (ultrawide), PDAF  \\nTOF 3D LiDAR scanner (depth)  \\nFeatures  Dual-LED dual -tone flash, HDR (photo/panorama)  \\nVideo  4K@24/30/60fps, 1080p@30/60/120/240fps, 10 -bit HDR, Dolby Vision HDR (up \\nto 60fps), ProRes, Cinematic mode, stereo sound rec.  \\nSELFIE \\nCAMERA  Dual  12 MP, f/2.2, 23mm (wide), 1/3.6\"  \\nSL 3D, (depth/biometrics sensor)  \\nFeatures  HDR  \\nVideo  4K@24/25/30/60fps, 1080p@30/60/120fps, gyro -EIS \\nSOUND  Loudspeaker  Yes, with stereo speakers  \\n3.5mm  jack  No \\nCOMMS  WLAN  Wi-Fi 802.11 a/b/g/n/ac/6, dual -band, hotspot  \\nBluetooth  5.0, A2DP, LE  \\nGPS  Yes, with A -GPS, GLONASS, GALILEO, BDS, QZSS  \\nNFC  Yes \\nRadio  No \\nUSB  Lightning, USB 2.0  \\nSensors  Face ID, accelerometer, gyro, proximity, compass, barometer', metadata={'page': 0, 'source': '.\\\\docs\\\\iphone13pm.pdf'}),\n",
       " Document(page_content='iPhone 13 Pro  Max  Specifications  \\nNETWORK  Technology  GSM  / CDMA  / HSPA  / EVDO  / LTE / 5G    \\nLAUNCH  Announced  2021, September 14  \\nStatus  Release 2021, September 24  \\nBODY  Dimensions  160.8 x 78.1 x 7.7 mm (6.33 x 3.07 x 0.30 in)  \\nWeight  240 g (8.47 oz)  \\nBuild  Glass front (Gorilla Glass), glass back (Gorilla Glass), stainless steel frame  \\nSIM  Single SIM (Nano -SIM and/or eSIM) or Dual SIM (Nano -SIM/eSIM, dual stand -by) \\n  IP68 dust/water resistant (up to 6m for 30 mins)  \\nApple Pay (Visa, MasterCard, AMEX certified)  \\nDISPLAY  Type  Super Retina XDR OLED, 120Hz, HDR10, Dolby Vision, 1000 nits (typ), 1200 nits \\n(peak)  \\nSize  6.7 inches, 109.8 cm2 (~87.4% screen -to-body ratio)  \\nResolution  1284 x 2778 pixels, 19.5:9 ratio (~458 ppi density)  \\nProtection  Scratch -resistant ceramic glass, oleophobic coating  \\n  Wide color gamut  \\nTrue-tone \\nPLATFORM  OS  iOS 15  \\nChipset  Apple A15 Bionic (5 nm)  \\nCPU  Hexa -core (2x3.22 GHz + 4xX.X GHz)  \\nGPU  Apple GPU (5 -core graphics)  \\nMEMORY  Card  slot  No \\nInternal  128GB 6GB RAM, 256GB 6GB RAM, 512GB 6GB RAM, 1TB  \\n  NVMe  \\nMAIN \\nCAMERA  Quad  12 MP, f/1.5, 26mm (wide), 1.9µm, dual pixel PDAF, sensor -shift OIS  \\n12 MP, f/2.8, 77mm (telephoto), PDAF, OIS, 3x optical zoom  \\n12 MP, f/1.8, 13mm, 120˚ (ultrawide), PDAF  \\nTOF 3D LiDAR scanner (depth)  \\nFeatures  Dual-LED dual -tone flash, HDR (photo/panorama)  \\nVideo  4K@24/30/60fps, 1080p@30/60/120/240fps, 10 -bit HDR, Dolby Vision HDR (up \\nto 60fps), ProRes, Cinematic mode, stereo sound rec.  \\nSELFIE \\nCAMERA  Dual  12 MP, f/2.2, 23mm (wide), 1/3.6\"  \\nSL 3D, (depth/biometrics sensor)  \\nFeatures  HDR  \\nVideo  4K@24/25/30/60fps, 1080p@30/60/120fps, gyro -EIS \\nSOUND  Loudspeaker  Yes, with stereo speakers  \\n3.5mm  jack  No \\nCOMMS  WLAN  Wi-Fi 802.11 a/b/g/n/ac/6, dual -band, hotspot  \\nBluetooth  5.0, A2DP, LE  \\nGPS  Yes, with A -GPS, GLONASS, GALILEO, BDS, QZSS  \\nNFC  Yes \\nRadio  No \\nUSB  Lightning, USB 2.0  \\nSensors  Face ID, accelerometer, gyro, proximity, compass, barometer', metadata={'page': 0, 'source': '.\\\\docs\\\\iphone13pm.pdf'}),\n",
       " Document(page_content='iPhone 13 Pro  Max  Specifications  \\nNETWORK  Technology  GSM  / CDMA  / HSPA  / EVDO  / LTE / 5G    \\nLAUNCH  Announced  2021, September 14  \\nStatus  Release 2021, September 24  \\nBODY  Dimensions  160.8 x 78.1 x 7.7 mm (6.33 x 3.07 x 0.30 in)  \\nWeight  240 g (8.47 oz)  \\nBuild  Glass front (Gorilla Glass), glass back (Gorilla Glass), stainless steel frame  \\nSIM  Single SIM (Nano -SIM and/or eSIM) or Dual SIM (Nano -SIM/eSIM, dual stand -by) \\n  IP68 dust/water resistant (up to 6m for 30 mins)  \\nApple Pay (Visa, MasterCard, AMEX certified)  \\nDISPLAY  Type  Super Retina XDR OLED, 120Hz, HDR10, Dolby Vision, 1000 nits (typ), 1200 nits \\n(peak)  \\nSize  6.7 inches, 109.8 cm2 (~87.4% screen -to-body ratio)  \\nResolution  1284 x 2778 pixels, 19.5:9 ratio (~458 ppi density)  \\nProtection  Scratch -resistant ceramic glass, oleophobic coating  \\n  Wide color gamut  \\nTrue-tone \\nPLATFORM  OS  iOS 15  \\nChipset  Apple A15 Bionic (5 nm)  \\nCPU  Hexa -core (2x3.22 GHz + 4xX.X GHz)  \\nGPU  Apple GPU (5 -core graphics)  \\nMEMORY  Card  slot  No \\nInternal  128GB 6GB RAM, 256GB 6GB RAM, 512GB 6GB RAM, 1TB  \\n  NVMe  \\nMAIN \\nCAMERA  Quad  12 MP, f/1.5, 26mm (wide), 1.9µm, dual pixel PDAF, sensor -shift OIS  \\n12 MP, f/2.8, 77mm (telephoto), PDAF, OIS, 3x optical zoom  \\n12 MP, f/1.8, 13mm, 120˚ (ultrawide), PDAF  \\nTOF 3D LiDAR scanner (depth)  \\nFeatures  Dual-LED dual -tone flash, HDR (photo/panorama)  \\nVideo  4K@24/30/60fps, 1080p@30/60/120/240fps, 10 -bit HDR, Dolby Vision HDR (up \\nto 60fps), ProRes, Cinematic mode, stereo sound rec.  \\nSELFIE \\nCAMERA  Dual  12 MP, f/2.2, 23mm (wide), 1/3.6\"  \\nSL 3D, (depth/biometrics sensor)  \\nFeatures  HDR  \\nVideo  4K@24/25/30/60fps, 1080p@30/60/120fps, gyro -EIS \\nSOUND  Loudspeaker  Yes, with stereo speakers  \\n3.5mm  jack  No \\nCOMMS  WLAN  Wi-Fi 802.11 a/b/g/n/ac/6, dual -band, hotspot  \\nBluetooth  5.0, A2DP, LE  \\nGPS  Yes, with A -GPS, GLONASS, GALILEO, BDS, QZSS  \\nNFC  Yes \\nRadio  No \\nUSB  Lightning, USB 2.0  \\nSensors  Face ID, accelerometer, gyro, proximity, compass, barometer', metadata={'page': 0, 'source': '.\\\\docs\\\\iphone13pm.pdf'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Given these documents, give me the difference between the iPhone 11 Pro Max and the iPhone 13 Pro Max's features.\"\n",
    "docs = vectordb.similarity_search(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0b27306",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_pipeline = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\",retriever=vectordb.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42b6a08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "answer = rag_pipeline(\"Given these documents, give me the difference between the iPhone 11 Pro Max and the iPhone 13 Pro Max's features using only these documents and not prior knowledge.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3e6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the dimensions of iPhone 11 Pro Max and iPhone SE?\"\n",
    "answer2 =  rag_it(question)\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the display sizes of the iPhone 11 Pro Max and iPhone SE?\"\n",
    "answer =  rag_it(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff255c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the display of iPhone 11 Pro Max?\"\n",
    "answer =  rag_it(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the display sizes of the iPhone SE and iPhone 13 Pro Max?\"\n",
    "answer =  rag_it(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faf5736-b64f-4abf-9a7f-47a6ac5758c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:05.234003Z",
     "iopub.status.busy": "2024-06-05T04:45:05.234003Z",
     "iopub.status.idle": "2024-06-05T04:45:05.236618Z",
     "shell.execute_reply": "2024-06-05T04:45:05.236618Z",
     "shell.execute_reply.started": "2024-06-05T04:45:05.234003Z"
    }
   },
   "outputs": [],
   "source": [
    "# memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "# prompt_template = PromptTemplate.from_template('Use these documents to answer questions that I ask about them')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc89b1-47cc-4182-89b2-509e2e0202ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:05.236618Z",
     "iopub.status.busy": "2024-06-05T04:45:05.236618Z",
     "iopub.status.idle": "2024-06-05T04:45:05.248056Z",
     "shell.execute_reply": "2024-06-05T04:45:05.248056Z",
     "shell.execute_reply.started": "2024-06-05T04:45:05.236618Z"
    }
   },
   "outputs": [],
   "source": [
    "# llama_model = LLMChain(llm=llm, prompt=prompt_template)\n",
    "# pdf_qa=ConversationalRetrievalChain.from_llm(llm=llm, retriever=vectordb.as_retriever(search_kwargs={'k':6}),verbose=False,memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6040c979-8703-4173-9625-382d3ad01272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:05.249061Z",
     "iopub.status.busy": "2024-06-05T04:45:05.248056Z",
     "iopub.status.idle": "2024-06-05T04:45:05.952022Z",
     "shell.execute_reply": "2024-06-05T04:45:05.952022Z",
     "shell.execute_reply.started": "2024-06-05T04:45:05.249061Z"
    }
   },
   "outputs": [],
   "source": [
    "# query=\"What is the main topic of the documents?\"\n",
    "# response=pdf_qa({\"question\": query})\n",
    "# print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17b1b68-3157-460f-8443-878d8016de78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T04:45:05.952022Z",
     "iopub.status.busy": "2024-06-05T04:45:05.952022Z",
     "iopub.status.idle": "2024-06-05T04:45:07.750183Z",
     "shell.execute_reply": "2024-06-05T04:45:07.750183Z",
     "shell.execute_reply.started": "2024-06-05T04:45:05.952022Z"
    }
   },
   "outputs": [],
   "source": [
    "# query=\"Given these documents, give me the difference between the iPhone 11 Pro max and the iPhone 13 Pro Max features.\"\n",
    "# response=pdf_qa({\"question\": query})\n",
    "# print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc60b60-b521-4ae4-ace1-b712258419f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
