{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_question =\"what are the weights of iPhone 13 pro max and iPhone SE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from langchain.llms  import LlamaCpp\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "#from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "#from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "import json\n",
    "#from langchain.schema import SystemMessage\n",
    "import cv2\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import pdf2image\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.schema import Document\n",
    "import asyncio\n",
    "from langchain.chains import RetrievalQA\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \".\\\\llms\\\\Meta-Llama-3-8B-Instruct.Q8_0.gguf\" #path to gguf quantized llama 3 model\n",
    "PERSIST_DIR = '.\\\\data' #path to where the chromadb is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class handles creating an instance of a llama, in other words, each object created is a llama 3 instance\n",
    "\n",
    "class MyLlama:\n",
    "    #useful parameters to know:\n",
    "    #model_path is where model is stored\n",
    "    #prompt is not part of the actual LlamaCpp Llama3 initialization, but for the sake of the other functions below, it is what it takes to answer questions\n",
    "    #lower temperature means model is less creative and less wild\n",
    "    #n_ctx is number of tokens for context size\n",
    "    #n_batch should be set to > 520 to use gpu, if verbose is set to true, you should see BLAS=1 when using gpu, adjust gpu layers as high as possible to not involve cpu\n",
    "    #max_tokens is for output token size\n",
    "\n",
    "    def __init__(self,model_path,prompt=None,temperature=10e-10, n_ctx=8192, n_batch=521, max_tokens=2000, n_gpu_layers=80):\n",
    "        self.prompt = prompt\n",
    "        self.llm = LlamaCpp(model_path=model_path,temperature=temperature,n_gpu_layers= n_gpu_layers,\n",
    "        main_gpu= 0,\n",
    "        vocab_only = False,\n",
    "        use_mmap = True,\n",
    "        use_mlock = False,\n",
    "        # Context Params                                                                                                                                                                                                                                                          \n",
    "        seed=  0xFFFFFFFF,\n",
    "        n_ctx = n_ctx,\n",
    "        n_batch = n_batch,\n",
    "        n_threads= None,\n",
    "        rope_freq_base = 0.0,\n",
    "        rope_freq_scale = 0.0,\n",
    "        f16_kv = True,\n",
    "        logits_all = False,\n",
    "        embedding = False,\n",
    "        # Sampling Params                                                                                                                                                                                                                                                         \n",
    "        last_n_tokens_size = 64,\n",
    "        # LoRA Params                                                                                                                                                                                                                                                             \n",
    "        lora_base = None,\n",
    "        lora_scale = 1.0,\n",
    "        lora_path = None,\n",
    "        # Backend Params                                                                                                                                                                                                                                                          \n",
    "        numa= False,\n",
    "        # Chat Format Params                                                                                                                                                                                                                                                      \n",
    "        # Misc                                                                                                                                                                                                                                                                    \n",
    "        verbose = False,\n",
    "        max_tokens = max_tokens)\n",
    "    def prompt_llama(self, **kwargs):\n",
    "         #llm should answer this prompt\n",
    "         message = self.prompt.format(**kwargs)\n",
    "         print(message)\n",
    "         return self.llm(message)\n",
    "    def set_prompt(self, prompt):\n",
    "        #sets prompt for the particular instance\n",
    "        self.prompt = prompt\n",
    "    def get_llm(self):\n",
    "        #just returns the raw llm (used later)\n",
    "        return self.llm\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the vector database to store pdf documents\n",
    "\n",
    "\n",
    "def get_product_name(text):\n",
    "    \"\"\"Supposed to get the product name given description of the product's specs.\"\"\"\n",
    "    prompt = \"\"\"Given this text, only output one specific product name and nothing else; no code or reasoning.\n",
    "    ---------------------------\n",
    "    {text}\n",
    "    ---------------------------\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    new_llm = MyLlama(prompt=prompt,model_path=MODEL_PATH)\n",
    "    product_name = new_llm.prompt_llama(text = text)\n",
    "    product_name = product_name.split( '```')[0].split('\\n')[0].lstrip()\n",
    "    return product_name\n",
    "    #use llama model here to ask this question\n",
    "\n",
    "\n",
    "def generate_question_source_pairs(question,tools):\n",
    "    \"\"\"This function generates the sub-question and the data-source that would answer the question.\"\"\"\n",
    "    sub_question_prompt=\"\"\"\\\n",
    "    You are an assistant that does not want to annoy the user, please do exactly as the user has asked below or you will be penalized!!!\n",
    "    The user demands a single list containing list(s) to be returned, please follow this when you output your answer\n",
    "\n",
    "    These are the guidelines you consider when completing your task:\n",
    "    - You should output at most one Python list of question-tool pairs (one for each tool and no more). Sample output: [[\"What is A?\", \"tool_name\"]].\n",
    "    - You can keep the original question if it's straightforward\n",
    "    - You should break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long)\n",
    "    - Please keep the subquestion-tool pair lists within one list!\n",
    "    - Each sub-question must ask about only ONE product!\n",
    "    - Each tool name in the output must be part of the tools GIVEN!\n",
    "    - Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name\n",
    "    - You can generate multiple sub-questions for each tool\n",
    "    - You don't need to use a tool if it's irrelevant\n",
    "    - Don't generate too many sub-questions. Only generate necessary ones.\n",
    "    Just the output and no code or text or symbol explanation afterwards please!\n",
    "\n",
    "    ## Examples of Question and their respective Subquestions:\n",
    "\n",
    "    Question: Compare Product A and Product B \n",
    "    Subquestions: [[\"Product overview of A\", \"product_a\"], [\"Product overview of B\", \"product_b\"]]\n",
    "\n",
    "    Question: Compare Product A and Product B in terms of scalability\n",
    "    Subquestions: [[\"Scalability of A\", \"product_a\"], [\"Scalability of B\", \"product_b\"]]\n",
    "\n",
    "    Question: What is a feature of Product A?\n",
    "    Subquestions: [[\"Feature of A\", \"product_a\"]]\n",
    "\n",
    "    ## Tools\n",
    "    {tools}\n",
    "\n",
    "    ##Question\n",
    "    {question}\n",
    "\n",
    "    ##Output\n",
    "    Please output here and follow the instructions that the user has mentioned.\n",
    "    \"\"\"\n",
    "    new_llm = MyLlama(prompt=sub_question_prompt, model_path=MODEL_PATH)\n",
    "    sub_pairs = new_llm.prompt_llama(question = question, tools = tools)\n",
    "    return sub_pairs\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def generate_data_idxs(db, question):\n",
    "    \"\"\"This function generates which data sources pertain to the questions given all the data sources.\"\"\"\n",
    "    dbs = db.similarity_search(question, k=20)\n",
    "    dbs_str = \"\".join(\n",
    "        f\"{database.metadata['collection_name']}: {database.page_content} \" for database in dbs\n",
    "    )\n",
    "    return dbs_str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
    "\n",
    "\n",
    "\n",
    "#So what these lines do:\n",
    "#Gathers the pdfs, chunks them after loading them then stores it into chroma db and names it after that pdf file name\n",
    "#for easier search I also introduced a dictionary\n",
    "\n",
    "collection_dict = {}\n",
    "collection_to_db = {}\n",
    "\n",
    "for file in os.listdir(\"docs\"):\n",
    "    pdf_path = \".\\\\docs\\\\\"+file\n",
    "    loader = UnstructuredPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size = 2000, chunk_overlap=300)\n",
    "    output_docs = text_splitter.split_documents(docs)\n",
    "    output_docs = filter_complex_metadata(output_docs)\n",
    "    for document in output_docs:\n",
    "        if \"source\" in document.metadata:\n",
    "            document.metadata = {\n",
    "                **document.metadata,\n",
    "                \"filename\": os.path.basename(document.metadata[\"source\"]),\n",
    "            }\n",
    "    output_docs = [doc for doc in output_docs]\n",
    "    product_name = get_product_name(output_docs[0].page_content)\n",
    "    collection_name = Path(file).stem\n",
    "    print(\"Collection name: \", collection_name)\n",
    "    vectordb = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "        embedding_function=embedding_model,\n",
    "    )\n",
    "    #adding the document to general and product-specific index\n",
    "    # general_vectordb.add_documents(output_docs)\n",
    "    vectordb.add_documents(output_docs)\n",
    "    collection_to_db[collection_name] = vectordb\n",
    "    collection_dict[collection_name] = product_name\n",
    "\n",
    "\n",
    "with open(\"collection_dict.json\", \"w\") as f:\n",
    "    json.dump(collection_dict,f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "#loading the files back into the collection\n",
    "with open(\"collection_dict.json\", \"r\") as f:\n",
    "    collection_dict = json.load(f)\n",
    "\n",
    "#creating a vectordb that indexes into the proper collection\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"index\",\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embedding_model,\n",
    ")\n",
    "\n",
    "#this is how the mapping occurs\n",
    "vectordb.add_documents([Document(page_content=value, metadata={\"collection_name\": key}) for key, value in collection_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just to show the collection_dict's keys and values very clearly\n",
    "print(collection_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#database stuff\n",
    "\n",
    "question = \"Given these documents, give me the difference between the iPhone 11 Pro Max and the iPhone 13 Pro Max's features using only these documents and not prior knowledge.\"\n",
    "question1 = \"What are the dimensions of iPhone 11 Pro Max and iPhone SE?\"\n",
    "question2 = \"What are the display sizes of the iPhone 11 Pro Max and iPhone SE?\"\n",
    "question3 = \"What is the display of iPhone 11 Pro Max?\"\n",
    "question4 = \"What are the display sizes of the iPhone SE and iPhone 13 Pro Max?\"\n",
    "\n",
    "# curr_question = question1\n",
    "\n",
    "tools_str = generate_data_idxs(db=vectordb, question=curr_question) #tools string gives filename to product name mapping\n",
    "print(\"tools_str: \",tools_str)\n",
    "sub_pairs = generate_question_source_pairs(question=curr_question, tools=tools_str) #generates [question, product] broken up for each question\n",
    "print(sub_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sub_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isolate the sub-question-tool pairs from text output (just pre-processing)\n",
    "original_str = ''\n",
    "for char in sub_pairs:\n",
    "    if char == \"]\" and original_str[-1] == \"]\":\n",
    "        original_str = original_str + char\n",
    "        break\n",
    "    if char != \"'\" or char != \" \":\n",
    "        original_str = original_str + char\n",
    "list_of_subqs = json.loads(original_str)\n",
    "print(list_of_subqs)\n",
    "print(\"type: \", type(list_of_subqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rag_it(question, db,llm):\n",
    "    \"\"\"Given a question return an answer using the rag pipeline\"\"\"\n",
    "    question = question[0:-1] + \"using only the documents given and no prior knowledge\"\n",
    "    docs = db.similarity_search(question)\n",
    "    rag_pipeline = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",retriever=db.as_retriever())\n",
    "    return (rag_pipeline(question))['result']\n",
    "\n",
    "async def really_answer_subqs(subquestions):\n",
    "    #for each subquestion, we pass it to the rag_it function for an answer\n",
    "    #each of these is async, so kind of execute at the same time\n",
    "    #then we wait to return the answer\n",
    "    async_tasks = []\n",
    "    llm = MyLlama(model_path=MODEL_PATH).get_llm()\n",
    "    for subquestion in subquestions:\n",
    "        db = collection_to_db[subquestion[1]]\n",
    "        async_tasks.append(rag_it(subquestion[0], db, llm))\n",
    "    answers = await asyncio.gather(*async_tasks)\n",
    "    #print(\"answers: \", answers)\n",
    "    return answers\n",
    "\n",
    "\n",
    "\n",
    "def answer_subqs(subq_list):\n",
    "    #we need to wrap the above function so that it is async\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    result = loop.run_until_complete(really_answer_subqs(subquestions=subq_list))\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#list of answers from answering subquestions\n",
    "answer_list = answer_subqs(list_of_subqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gathers all the answers and with some string processing, returns final answer\n",
    "unformatted_str = ''\n",
    "for answer in answer_list:\n",
    "    unformatted_str = unformatted_str + ' '+ answer\n",
    "print(\"Question: \", curr_question)\n",
    "print(\"Final Answer: \", unformatted_str.lstrip())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
