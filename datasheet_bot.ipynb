{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_question =\"what are the weights of iPhone 13 pro max and iPhone SE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from langchain.llms  import LlamaCpp\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "#from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "#from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "import json\n",
    "#from langchain.schema import SystemMessage\n",
    "import cv2\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import numpy as np\n",
    "import pdf2image\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "from langchain.document_loaders import UnstructuredPDFLoader\n",
    "from langchain.schema import Document\n",
    "import asyncio\n",
    "from langchain.chains import RetrievalQA\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.3 | packaged by Anaconda, Inc. | (main, May  6 2024, 19:42:21) [MSC v.1916 64 bit (AMD64)]\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "print(sys.prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \".\\\\llms\\\\Meta-Llama-3-8B-Instruct.Q8_0.gguf\" #path to gguf quantized llama 3 model\n",
    "PERSIST_DIR = '.\\\\data' #path to where the chromadb is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this class handles creating an instance of a llama, in other words, each object created is a llama 3 instance\n",
    "\n",
    "class MyLlama:\n",
    "    #useful parameters to know:\n",
    "    #model_path is where model is stored\n",
    "    #prompt is not part of the actual LlamaCpp Llama3 initialization, but for the sake of the other functions below, it is what it takes to answer questions\n",
    "    #lower temperature means model is less creative and less wild\n",
    "    #n_ctx is number of tokens for context size\n",
    "    #n_batch should be set to > 520 to use gpu, if verbose is set to true, you should see BLAS=1 when using gpu, adjust gpu layers as high as possible to not involve cpu\n",
    "    #max_tokens is for output token size\n",
    "\n",
    "    def __init__(self,model_path,prompt=None,temperature=10e-10, n_ctx=8192, n_batch=521, max_tokens=2000, n_gpu_layers=80):\n",
    "        self.prompt = prompt\n",
    "        self.llm = LlamaCpp(model_path=model_path,temperature=temperature,n_gpu_layers= n_gpu_layers,\n",
    "        main_gpu= 0,\n",
    "        vocab_only = False,\n",
    "        use_mmap = True,\n",
    "        use_mlock = False,\n",
    "        # Context Params                                                                                                                                                                                                                                                          \n",
    "        seed=  0xFFFFFFFF,\n",
    "        n_ctx = n_ctx,\n",
    "        n_batch = n_batch,\n",
    "        n_threads= None,\n",
    "        rope_freq_base = 0.0,\n",
    "        rope_freq_scale = 0.0,\n",
    "        f16_kv = True,\n",
    "        logits_all = False,\n",
    "        embedding = False,\n",
    "        # Sampling Params                                                                                                                                                                                                                                                         \n",
    "        last_n_tokens_size = 64,\n",
    "        # LoRA Params                                                                                                                                                                                                                                                             \n",
    "        lora_base = None,\n",
    "        lora_scale = 1.0,\n",
    "        lora_path = None,\n",
    "        # Backend Params                                                                                                                                                                                                                                                          \n",
    "        numa= False,\n",
    "        # Chat Format Params                                                                                                                                                                                                                                                      \n",
    "        # Misc                                                                                                                                                                                                                                                                    \n",
    "        verbose = False,\n",
    "        max_tokens = max_tokens)\n",
    "    def prompt_llama(self, **kwargs):\n",
    "         #llm should answer this prompt\n",
    "         message = self.prompt.format(**kwargs)\n",
    "         print(message)\n",
    "         return self.llm(message)\n",
    "    def set_prompt(self, prompt):\n",
    "        #sets prompt for the particular instance\n",
    "        self.prompt = prompt\n",
    "    def get_llm(self):\n",
    "        #just returns the raw llm (used later)\n",
    "        return self.llm\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the vector database to store pdf documents\n",
    "\n",
    "\n",
    "def get_product_name(text):\n",
    "    \"\"\"Supposed to get the product name given description of the product's specs.\"\"\"\n",
    "    prompt = \"\"\"Given this text, only output one specific product name and nothing else; no code or reasoning.\n",
    "    ---------------------------\n",
    "    {text}\n",
    "    ---------------------------\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    new_llm = MyLlama(prompt=prompt,model_path=MODEL_PATH)\n",
    "    product_name = new_llm.prompt_llama(text = text)\n",
    "    product_name = product_name.split( '```')[0].split('\\n')[0].lstrip()\n",
    "    return product_name\n",
    "    #use llama model here to ask this question\n",
    "\n",
    "\n",
    "def generate_question_source_pairs(question,tools):\n",
    "    \"\"\"This function generates the sub-question and the data-source that would answer the question.\"\"\"\n",
    "    sub_question_prompt=\"\"\"\\\n",
    "    You are an assistant that does not want to annoy the user, please do exactly as the user has asked below or you will be penalized!!!\n",
    "    The user demands a single list containing list(s) to be returned, please follow this when you output your answer\n",
    "\n",
    "    These are the guidelines you consider when completing your task:\n",
    "    - You should output at most one Python list of question-tool pairs (one for each tool and no more). Sample output: [[\"What is A?\", \"tool_name\"]].\n",
    "    - You can keep the original question if it's straightforward\n",
    "    - You should break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long)\n",
    "    - Please keep the subquestion-tool pair lists within one list!\n",
    "    - Each sub-question must ask about only ONE product!\n",
    "    - Each tool name in the output must be part of the tools GIVEN!\n",
    "    - Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name\n",
    "    - You can generate multiple sub-questions for each tool\n",
    "    - You don't need to use a tool if it's irrelevant\n",
    "    - Don't generate too many sub-questions. Only generate necessary ones.\n",
    "    Just the output and no code or text or symbol explanation afterwards please!\n",
    "\n",
    "    ## Examples of Question and their respective Subquestions:\n",
    "\n",
    "    Question: Compare Product A and Product B \n",
    "    Subquestions: [[\"Product overview of A\", \"product_a\"], [\"Product overview of B\", \"product_b\"]]\n",
    "\n",
    "    Question: Compare Product A and Product B in terms of scalability\n",
    "    Subquestions: [[\"Scalability of A\", \"product_a\"], [\"Scalability of B\", \"product_b\"]]\n",
    "\n",
    "    Question: What is a feature of Product A?\n",
    "    Subquestions: [[\"Feature of A\", \"product_a\"]]\n",
    "\n",
    "    ## Tools\n",
    "    {tools}\n",
    "\n",
    "    ##Question\n",
    "    {question}\n",
    "\n",
    "    ##Output\n",
    "    Please output here and follow the instructions that the user has mentioned.\n",
    "    \"\"\"\n",
    "    new_llm = MyLlama(prompt=sub_question_prompt, model_path=MODEL_PATH)\n",
    "    sub_pairs = new_llm.prompt_llama(question = question, tools = tools)\n",
    "    return sub_pairs\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def generate_data_idxs(db, question):\n",
    "    \"\"\"This function generates which data sources pertain to the questions given all the data sources.\"\"\"\n",
    "    dbs = db.similarity_search(question, k=20)\n",
    "    dbs_str = \"\".join(\n",
    "        f\"{database.metadata['collection_name']}: {database.page_content} \" for database in dbs\n",
    "    )\n",
    "    return dbs_str\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! main_gpu is not default parameter.\n",
      "                main_gpu was transferred to model_kwargs.\n",
      "                Please confirm that main_gpu is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! embedding is not default parameter.\n",
      "                embedding was transferred to model_kwargs.\n",
      "                Please confirm that embedding is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! lora_scale is not default parameter.\n",
      "                lora_scale was transferred to model_kwargs.\n",
      "                Please confirm that lora_scale is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! numa is not default parameter.\n",
      "                numa was transferred to model_kwargs.\n",
      "                Please confirm that numa is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given this text, only output one specific product name and nothing else; no code or reasoning.\n",
      "    ---------------------------\n",
      "    IPhone 11 Pro Max Specifications\n",
      "\n",
      "NETWORK Technology\n",
      "\n",
      "GSM / CDMA / HSPA / EVDO / LTE\n",
      "\n",
      "LAUNCH Announced\n",
      "\n",
      "Status\n",
      "\n",
      "2019, September Coming soon. Exp. release 2019, September\n",
      "\n",
      "BODY\n",
      "\n",
      "Dimensions\n",
      "\n",
      "Weight\n",
      "\n",
      "Build\n",
      "\n",
      "158 x 77.8 x 8.1 mm (6.22 x 3.06 x 0.32 in) 226 g (7.97 oz) Front/back glass, stainless steel frame\n",
      "\n",
      "SIM\n",
      "\n",
      "Single SIM (Nano-SIM and/or Electronic SIM card) or Dual SIM (Nano-SIM, dual stand-by) - for China\n",
      "\n",
      "IP68 dust/water resistant (up to 4m for 30 mins) Apple Pay (Visa, MasterCard, AMEX certified)\n",
      "\n",
      "DISPLAY Type Size\n",
      "\n",
      "Super Retina XDR OLED capacitive touchscreen, 16M colors 6.5 inches, 102.9 cm2 (~83.7% screen-to-body ratio)\n",
      "\n",
      "Resolution Protection\n",
      "\n",
      "1242 x 2688 pixels, 19.5:9 ratio (~458 ppi density) Scratch-resistant glass, oleophobic coating\n",
      "\n",
      "800 nits Dolby Vision HDR10 Wide color gamut True-tone 120 Hz touch-sensing\n",
      "\n",
      "PLATFORM OS\n",
      "\n",
      "Chipset\n",
      "\n",
      "iOS 13 Apple A13 Bionic (7 nm+)\n",
      "\n",
      "MEMORY Card slot\n",
      "\n",
      "Internal\n",
      "\n",
      "No 64GB 6GB RAM, 256GB 6GB RAM, 512GB 6GB RAM\n",
      "\n",
      "MAIN CAMERA\n",
      "\n",
      "Triple\n",
      "\n",
      "12 MP, f/1.8, 26mm (wide), 1/2.55\", 1.4µm, PDAF, OIS 12 MP, f/2.0, 52mm (telephoto), 1/3.4\", 1.0µm, PDAF, OIS, 2x optical zoom 12 MP, f/2.4, 13mm (ultrawide)\n",
      "\n",
      "Features Video\n",
      "\n",
      "Quad-LED dual-tone flash, HDR (photo/panorama) 2160p@24/30/60fps, 1080p@30/60/120/240fps, HDR, stereo sound rec.\n",
      "\n",
      "SELFIE CAMERA\n",
      "\n",
      "Dual\n",
      "\n",
      "Features\n",
      "\n",
      "12 MP, f/2.2 TOF 3D camera HDR\n",
      "\n",
      "Video\n",
      "\n",
      "2160p@24/30/60fps, 1080p@30/60/120fps, gyro-EIS\n",
      "\n",
      "SOUND\n",
      "\n",
      "Loudspeaker\n",
      "\n",
      "Yes, with stereo speakers\n",
      "\n",
      "3.5mm jack\n",
      "\n",
      "No\n",
      "\n",
      "Active noise cancellation with dedicated mic Dolby Atmos Dolby Digital Plus\n",
      "\n",
      "COMMS WLAN\n",
      "\n",
      "Bluetooth GPS\n",
      "\n",
      "Wi-Fi 802.11 a/b/g/n/ac/ax, dual-band, hotspot 5.0, A2DP, LE Yes, with A-GPS, GLONASS, GALILEO, QZSS\n",
      "\n",
      "IPhone 11 Pro Max Specifications\n",
      "\n",
      "NFC Radio\n",
      "\n",
      "USB\n",
      "\n",
      "Yes No 2.0, proprietary reversible connector\n",
      "\n",
      "FEATURES Sensors\n",
      "\n",
      "Face ID, accelerometer, gyro, proximity, compass, barometer Siri natural language commands and dictation\n",
      "\n",
      "BATTERY\n",
      "\n",
      "Non-removable Li-Ion 3500 mAh battery\n",
      "\n",
      "Charging\n",
      "\n",
      "Talk time\n",
      "    ---------------------------\n",
      "    Answer:\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection name:  ip11pmaxspecs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! main_gpu is not default parameter.\n",
      "                main_gpu was transferred to model_kwargs.\n",
      "                Please confirm that main_gpu is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! embedding is not default parameter.\n",
      "                embedding was transferred to model_kwargs.\n",
      "                Please confirm that embedding is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! lora_scale is not default parameter.\n",
      "                lora_scale was transferred to model_kwargs.\n",
      "                Please confirm that lora_scale is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! numa is not default parameter.\n",
      "                numa was transferred to model_kwargs.\n",
      "                Please confirm that numa is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given this text, only output one specific product name and nothing else; no code or reasoning.\n",
      "    ---------------------------\n",
      "    iPhone 13 Pro Max Specifications\n",
      "\n",
      "NETWORK Technology LAUNCH Announced\n",
      "\n",
      "BODY\n",
      "\n",
      "Status Dimensions Weight Build SIM\n",
      "\n",
      "DISPLAY Type\n",
      "\n",
      "Size Resolution Protection\n",
      "\n",
      "PLATFORM OS\n",
      "\n",
      "Chipset CPU GPU\n",
      "\n",
      "GSM / CDMA / HSPA / EVDO / LTE / 5G\n",
      "\n",
      "2021, September 14 Release 2021, September 24 160.8 x 78.1 x 7.7 mm (6.33 x 3.07 x 0.30 in) 240 g (8.47 oz) Glass front (Gorilla Glass), glass back (Gorilla Glass), stainless steel frame Single SIM (Nano-SIM and/or eSIM) or Dual SIM (Nano-SIM/eSIM, dual stand-by) IP68 dust/water resistant (up to 6m for 30 mins) Apple Pay (Visa, MasterCard, AMEX certified) Super Retina XDR OLED, 120Hz, HDR10, Dolby Vision, 1000 nits (typ), 1200 nits (peak) 6.7 inches, 109.8 cm2 (~87.4% screen-to-body ratio) 1284 x 2778 pixels, 19.5:9 ratio (~458 ppi density) Scratch-resistant ceramic glass, oleophobic coating Wide color gamut True-tone iOS 15 Apple A15 Bionic (5 nm) Hexa-core (2x3.22 GHz + 4xX.X GHz) Apple GPU (5-core graphics)\n",
      "\n",
      "MEMORY Card slot\n",
      "\n",
      "Internal\n",
      "\n",
      "MAIN CAMERA\n",
      "\n",
      "Quad\n",
      "\n",
      "Features Video\n",
      "\n",
      "SELFIE CAMERA\n",
      "\n",
      "Dual\n",
      "\n",
      "SOUND\n",
      "\n",
      "Features Video Loudspeaker 3.5mm jack\n",
      "\n",
      "COMMS WLAN\n",
      "\n",
      "Bluetooth GPS NFC Radio USB\n",
      "\n",
      "No 128GB 6GB RAM, 256GB 6GB RAM, 512GB 6GB RAM, 1TB NVMe 12 MP, f/1.5, 26mm (wide), 1.9µm, dual pixel PDAF, sensor-shift OIS 12 MP, f/2.8, 77mm (telephoto), PDAF, OIS, 3x optical zoom 12 MP, f/1.8, 13mm, 120˚ (ultrawide), PDAF TOF 3D LiDAR scanner (depth) Dual-LED dual-tone flash, HDR (photo/panorama) 4K@24/30/60fps, 1080p@30/60/120/240fps, 10-bit HDR, Dolby Vision HDR (up to 60fps), ProRes, Cinematic mode, stereo sound rec. 12 MP, f/2.2, 23mm (wide), 1/3.6\" SL 3D, (depth/biometrics sensor) HDR 4K@24/25/30/60fps, 1080p@30/60/120fps, gyro-EIS Yes, with stereo speakers No Wi-Fi 802.11 a/b/g/n/ac/6, dual-band, hotspot 5.0, A2DP, LE Yes, with A-GPS, GLONASS, GALILEO, BDS, QZSS Yes No Lightning, USB 2.0\n",
      "\n",
      "Sensors\n",
      "\n",
      "Face ID, accelerometer, gyro, proximity, compass, barometer\n",
      "\n",
      "iPhone 13 Pro Max Specifications\n",
      "\n",
      "FEATURES BATTERY Type\n",
      "\n",
      "Charging\n",
      "\n",
      "Stand-by Music play\n",
      "    ---------------------------\n",
      "    Answer:\n",
      "    \n",
      "Collection name:  iphone13pm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! main_gpu is not default parameter.\n",
      "                main_gpu was transferred to model_kwargs.\n",
      "                Please confirm that main_gpu is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! embedding is not default parameter.\n",
      "                embedding was transferred to model_kwargs.\n",
      "                Please confirm that embedding is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! lora_scale is not default parameter.\n",
      "                lora_scale was transferred to model_kwargs.\n",
      "                Please confirm that lora_scale is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! numa is not default parameter.\n",
      "                numa was transferred to model_kwargs.\n",
      "                Please confirm that numa is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given this text, only output one specific product name and nothing else; no code or reasoning.\n",
      "    ---------------------------\n",
      "    NETWORK Technology\n",
      "\n",
      "LAUNCH Announced\n",
      "\n",
      "Status\n",
      "\n",
      "BODY\n",
      "\n",
      "Dimensions\n",
      "\n",
      "Weight\n",
      "\n",
      "Build\n",
      "\n",
      "SIM\n",
      "\n",
      "DISPLAY Type Size\n",
      "\n",
      "Resolution\n",
      "\n",
      "Protection\n",
      "\n",
      "PLATFORM OS\n",
      "\n",
      "Chipset\n",
      "\n",
      "CPU\n",
      "\n",
      "GPU\n",
      "\n",
      "MEMORY Card slot\n",
      "\n",
      "Internal\n",
      "\n",
      "MAIN CAMERA\n",
      "\n",
      "Single\n",
      "\n",
      "Features\n",
      "\n",
      "Video\n",
      "\n",
      "SELFIE CAMERA\n",
      "\n",
      "Single\n",
      "\n",
      "Features\n",
      "\n",
      "Video\n",
      "\n",
      "SOUND\n",
      "\n",
      "Loudspeaker\n",
      "\n",
      "3.5mm jack\n",
      "\n",
      "COMMS WLAN\n",
      "\n",
      "Bluetooth\n",
      "\n",
      "GPS\n",
      "\n",
      "NFC\n",
      "\n",
      "Radio\n",
      "\n",
      "USB\n",
      "\n",
      "FEATURES Sensors\n",
      "\n",
      "iPhone SE Specifications\n",
      "\n",
      "GSM / CDMA / HSPA / EVDO / LTE\n",
      "\n",
      "2020, April 15\n",
      "\n",
      "Coming soon. Exp. release 2020, April 24\n",
      "\n",
      "138.4 x 67.3 x 7.3 mm (5.45 x 2.65 x 0.29 in)\n",
      "\n",
      "148 g (5.22 oz)\n",
      "\n",
      "Glass front, glass back, aluminum frame\n",
      "\n",
      "Nano-SIM and/or eSIM\n",
      "\n",
      "IP67 dust/water resistant (up to 1m for 30 mins) Apple Pay (Visa, MasterCard, AMEX certified)\n",
      "\n",
      "Retina IPS LCD capacitive touchscreen, 16M colors\n",
      "\n",
      "4.7 inches, 60.9 cm2 (~65.4% screen-to-body ratio)\n",
      "\n",
      "750 x 1334 pixels, 16:9 ratio (~326 ppi density)\n",
      "\n",
      "Ion-strengthened glass, oleophobic coating\n",
      "\n",
      "Wide color gamut True-tone 625 nits max brightness (advertised)\n",
      "\n",
      "iOS 13\n",
      "\n",
      "Apple A13 Bionic (7 nm+)\n",
      "\n",
      "Hexa-core (2x2.65 GHz Lightning + 4x1.8 GHz Thunder)\n",
      "\n",
      "Apple GPU (4-core graphics)\n",
      "\n",
      "No\n",
      "\n",
      "64GB 3GB RAM, 128GB 3GB RAM, 256GB 3GB RAM\n",
      "\n",
      "NVMe\n",
      "\n",
      "12 MP, f/1.8 (wide), PDAF, OIS\n",
      "\n",
      "Dual-LED dual-tone flash, HDR\n",
      "\n",
      "2160p@24/30/60fps, 1080p@30/60/120/240fps, HDR, OIS, stereo sound rec.\n",
      "\n",
      "7 MP, f/2.2\n",
      "\n",
      "Face detection, HDR, panorama\n",
      "\n",
      "1080p@30fps; gyro-EIS\n",
      "\n",
      "Yes, with stereo speakers\n",
      "\n",
      "No\n",
      "\n",
      "Wi-Fi 802.11 a/b/g/n/ac/ax, dual-band, hotspot\n",
      "\n",
      "5.0, A2DP, LE\n",
      "\n",
      "Yes, with A-GPS, GLONASS\n",
      "\n",
      "Yes\n",
      "\n",
      "No\n",
      "\n",
      "2.0, proprietary reversible connector\n",
      "\n",
      "Fingerprint (front-mounted), accelerometer, proximity, gyro, compass, barometer\n",
      "\n",
      "BATTERY\n",
      "\n",
      "MISC\n",
      "\n",
      "Charging\n",
      "\n",
      "Colors\n",
      "\n",
      "Models\n",
      "\n",
      "Price\n",
      "\n",
      "iPhone SE Specifications\n",
      "\n",
      "Siri natural language commands and dictation\n",
      "\n",
      "Non-removable Li-Ion 1821 mAh battery (6.96 Wh)\n",
      "\n",
      "Fast charging 18W, 50% in 30 min (advertised) Qi wireless charging\n",
      "\n",
      "Black, White, Red\n",
      "\n",
      "A2275, A2296, A2298\n",
      "\n",
      "$ 121.00 / £ 419.00 / € 479.00\n",
      "    ---------------------------\n",
      "    Answer:\n",
      "    \n",
      "Collection name:  iphoneSEspecs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# embedding_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "embedding_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
    "\n",
    "\n",
    "\n",
    "#So what these lines do:\n",
    "#Gathers the pdfs, chunks them after loading them then stores it into chroma db and names it after that pdf file name\n",
    "#for easier search I also introduced a dictionary\n",
    "\n",
    "collection_dict = {}\n",
    "collection_to_db = {}\n",
    "\n",
    "for file in os.listdir(\"docs\"):\n",
    "    pdf_path = \".\\\\docs\\\\\"+file\n",
    "    loader = UnstructuredPDFLoader(pdf_path)\n",
    "    docs = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(separator=\"\\n\\n\", chunk_size = 2000, chunk_overlap=300)\n",
    "    output_docs = text_splitter.split_documents(docs)\n",
    "    output_docs = filter_complex_metadata(output_docs)\n",
    "    for document in output_docs:\n",
    "        if \"source\" in document.metadata:\n",
    "            document.metadata = {\n",
    "                **document.metadata,\n",
    "                \"filename\": os.path.basename(document.metadata[\"source\"]),\n",
    "            }\n",
    "    output_docs = [doc for doc in output_docs]\n",
    "    product_name = get_product_name(output_docs[0].page_content)\n",
    "    collection_name = Path(file).stem\n",
    "    print(\"Collection name: \", collection_name)\n",
    "    vectordb = Chroma(\n",
    "        collection_name=collection_name,\n",
    "        persist_directory=PERSIST_DIR,\n",
    "        embedding_function=embedding_model,\n",
    "    )\n",
    "    #adding the document to general and product-specific index\n",
    "    # general_vectordb.add_documents(output_docs)\n",
    "    vectordb.add_documents(output_docs)\n",
    "    collection_to_db[collection_name] = vectordb\n",
    "    collection_dict[collection_name] = product_name\n",
    "\n",
    "\n",
    "with open(\"collection_dict.json\", \"w\") as f:\n",
    "    json.dump(collection_dict,f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ba6e3f66-a3e1-4fa1-9d53-55eae67e8964',\n",
       " '18c59664-86d1-4c7f-830c-80a333b31585',\n",
       " '36fd1013-f3fe-4319-a8cd-8e66b46a0262']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "#loading the files back into the collection\n",
    "with open(\"collection_dict.json\", \"r\") as f:\n",
    "    collection_dict = json.load(f)\n",
    "\n",
    "#creating a vectordb that indexes into the proper collection\n",
    "vectordb = Chroma(\n",
    "    collection_name=\"index\",\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embedding_model,\n",
    ")\n",
    "\n",
    "#this is how the mapping occurs\n",
    "vectordb.add_documents([Document(page_content=value, metadata={\"collection_name\": key}) for key, value in collection_dict.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ip11pmaxspecs': 'iPhone 11 Pro Max', 'iphone13pm': 'iPhone 13 Pro Max', 'iphoneSEspecs': 'iPhone SE'}\n"
     ]
    }
   ],
   "source": [
    "#just to show the collection_dict's keys and values very clearly\n",
    "print(collection_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tools_str:  iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! main_gpu is not default parameter.\n",
      "                main_gpu was transferred to model_kwargs.\n",
      "                Please confirm that main_gpu is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! embedding is not default parameter.\n",
      "                embedding was transferred to model_kwargs.\n",
      "                Please confirm that embedding is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! lora_scale is not default parameter.\n",
      "                lora_scale was transferred to model_kwargs.\n",
      "                Please confirm that lora_scale is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! numa is not default parameter.\n",
      "                numa was transferred to model_kwargs.\n",
      "                Please confirm that numa is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    You are an assistant that does not want to annoy the user, please do exactly as the user has asked below or you will be penalized!!!\n",
      "    The user demands a single list containing list(s) to be returned, please follow this when you output your answer\n",
      "\n",
      "    These are the guidelines you consider when completing your task:\n",
      "    - You should output at most one Python list of question-tool pairs (one for each tool and no more). Sample output: [[\"What is A?\", \"tool_name\"]].\n",
      "    - You can keep the original question if it's straightforward\n",
      "    - You should break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long)\n",
      "    - Please keep the subquestion-tool pair lists within one list!\n",
      "    - Each sub-question must ask about only ONE product!\n",
      "    - Each tool name in the output must be part of the tools GIVEN!\n",
      "    - Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name\n",
      "    - You can generate multiple sub-questions for each tool\n",
      "    - You don't need to use a tool if it's irrelevant\n",
      "    - Don't generate too many sub-questions. Only generate necessary ones.\n",
      "    Just the output and no code or text or symbol explanation afterwards please!\n",
      "\n",
      "    ## Examples of Question and their respective Subquestions:\n",
      "\n",
      "    Question: Compare Product A and Product B \n",
      "    Subquestions: [[\"Product overview of A\", \"product_a\"], [\"Product overview of B\", \"product_b\"]]\n",
      "\n",
      "    Question: Compare Product A and Product B in terms of scalability\n",
      "    Subquestions: [[\"Scalability of A\", \"product_a\"], [\"Scalability of B\", \"product_b\"]]\n",
      "\n",
      "    Question: What is a feature of Product A?\n",
      "    Subquestions: [[\"Feature of A\", \"product_a\"]]\n",
      "\n",
      "    ## Tools\n",
      "    iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max iphone13pm: iPhone 13 Pro Max \n",
      "\n",
      "    ##Question\n",
      "    what are the weights of iPhone 13 pro max and iPhone SE\n",
      "\n",
      "    ##Output\n",
      "    Please output here and follow the instructions that the user has mentioned.\n",
      "    \n",
      " [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]] \n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    tools = ['iphone13pm', 'iphone_se']\n",
      "    question = \"what are the weights of iPhone 13 pro max and iPhone SE\"\n",
      "    \n",
      "    if \"and\" in question:\n",
      "        products = question.split(\"and\")\n",
      "        subquestions = []\n",
      "        for product in products:\n",
      "            if \"iPhone\" in product:\n",
      "                tool_name = [tool for tool in tools if tool in product][0]\n",
      "                subquestion = f\"What is the weight of {product}?\"\n",
      "                subquestions.append([subquestion, tool_name])\n",
      "    else:\n",
      "        subquestions = []\n",
      "        for tool in tools:\n",
      "            subquestion = f\"What is the weight of {tool}?\"\n",
      "            subquestions.append([subquestion, tool])\n",
      "\n",
      "    return [subquestions]  # Output should be a list containing one list of question-tool pairs. \n",
      "# [[[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Correct output\n",
      "print([subquestions])  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output\n"
     ]
    }
   ],
   "source": [
    "#database stuff\n",
    "\n",
    "question = \"Given these documents, give me the difference between the iPhone 11 Pro Max and the iPhone 13 Pro Max's features using only these documents and not prior knowledge.\"\n",
    "question1 = \"What are the dimensions of iPhone 11 Pro Max and iPhone SE?\"\n",
    "question2 = \"What are the display sizes of the iPhone 11 Pro Max and iPhone SE?\"\n",
    "question3 = \"What is the display of iPhone 11 Pro Max?\"\n",
    "question4 = \"What are the display sizes of the iPhone SE and iPhone 13 Pro Max?\"\n",
    "\n",
    "# curr_question = question1\n",
    "\n",
    "tools_str = generate_data_idxs(db=vectordb, question=curr_question) #tools string gives filename to product name mapping\n",
    "print(\"tools_str: \",tools_str)\n",
    "sub_pairs = generate_question_source_pairs(question=curr_question, tools=tools_str) #generates [question, product] broken up for each question\n",
    "print(sub_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]] \n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    tools = ['iphone13pm', 'iphone_se']\n",
      "    question = \"what are the weights of iPhone 13 pro max and iPhone SE\"\n",
      "    \n",
      "    if \"and\" in question:\n",
      "        products = question.split(\"and\")\n",
      "        subquestions = []\n",
      "        for product in products:\n",
      "            if \"iPhone\" in product:\n",
      "                tool_name = [tool for tool in tools if tool in product][0]\n",
      "                subquestion = f\"What is the weight of {product}?\"\n",
      "                subquestions.append([subquestion, tool_name])\n",
      "    else:\n",
      "        subquestions = []\n",
      "        for tool in tools:\n",
      "            subquestion = f\"What is the weight of {tool}?\"\n",
      "            subquestions.append([subquestion, tool])\n",
      "\n",
      "    return [subquestions]  # Output should be a list containing one list of question-tool pairs. \n",
      "# [[[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Correct output\n",
      "print([subquestions])  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output your answer. \n",
      "# Please keep the original question if it's straightforward \n",
      "# You can break down a generic, ambiguous question into concrete sub-questions about documents in the database (please output this in one list and do not make it too long) \n",
      "# Each sub-question must ask about only ONE product! \n",
      "# Each tool name in the output must be part of the tools GIVEN! \n",
      "# Each output must follow the Sample output's format, no multiple tools, each pair must have only one sub-question and one tool name \n",
      "# You can generate multiple sub-questions for each tool \n",
      "# You don't need to use a tool if it's irrelevant \n",
      "# Don't generate too many sub-questions. Only generate necessary ones. \n",
      "# Just the output and no code or text or symbol explanation afterwards please!  # Please do not remove this line, it's for testing purposes only! \n",
      "# [ [[\"What is the weight of iPhone 13 Pro Max?\", \"iphone13pm\"], [\"What is the weight of iPhone SE?\", \"iphone_se\"]]]  # Expected output\n",
      "# The user demands a single list containing list(s) to be returned, please follow this when you output\n"
     ]
    }
   ],
   "source": [
    "print(sub_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['What is the weight of iPhone 13 Pro Max?', 'iphone13pm'], ['What is the weight of iPhone SE?', 'iphone_se']]\n",
      "type:  <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#isolate the sub-question-tool pairs from text output (just pre-processing)\n",
    "original_str = ''\n",
    "for char in sub_pairs:\n",
    "    if char == \"]\" and original_str[-1] == \"]\":\n",
    "        original_str = original_str + char\n",
    "        break\n",
    "    if char != \"'\" or char != \" \":\n",
    "        original_str = original_str + char\n",
    "list_of_subqs = json.loads(original_str)\n",
    "print(list_of_subqs)\n",
    "print(\"type: \", type(list_of_subqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def rag_it(question, db,llm):\n",
    "    \"\"\"Given a question return an answer using the rag pipeline\"\"\"\n",
    "    question = question[0:-1] + \"using only the documents given and no prior knowledge\"\n",
    "    docs = db.similarity_search(question)\n",
    "    rag_pipeline = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",retriever=db.as_retriever())\n",
    "    return (rag_pipeline(question))['result']\n",
    "\n",
    "async def really_answer_subqs(subquestions):\n",
    "    #for each subquestion, we pass it to the rag_it function for an answer\n",
    "    #each of these is async, so kind of execute at the same time\n",
    "    #then we wait to return the answer\n",
    "    async_tasks = []\n",
    "    llm = MyLlama(model_path=MODEL_PATH).get_llm()\n",
    "    for subquestion in subquestions:\n",
    "        db = collection_to_db[subquestion[1]]\n",
    "        async_tasks.append(rag_it(subquestion[0], db, llm))\n",
    "    answers = await asyncio.gather(*async_tasks)\n",
    "    #print(\"answers: \", answers)\n",
    "    return answers\n",
    "\n",
    "\n",
    "\n",
    "def answer_subqs(subq_list):\n",
    "    #we need to wrap the above function so that it is async\n",
    "    try:\n",
    "        loop = asyncio.get_event_loop()\n",
    "    except:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "    result = loop.run_until_complete(really_answer_subqs(subquestions=subq_list))\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! main_gpu is not default parameter.\n",
      "                main_gpu was transferred to model_kwargs.\n",
      "                Please confirm that main_gpu is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! embedding is not default parameter.\n",
      "                embedding was transferred to model_kwargs.\n",
      "                Please confirm that embedding is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! lora_scale is not default parameter.\n",
      "                lora_scale was transferred to model_kwargs.\n",
      "                Please confirm that lora_scale is what you intended.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\langchain_core\\utils\\utils.py:161: UserWarning: WARNING! numa is not default parameter.\n",
      "                numa was transferred to model_kwargs.\n",
      "                Please confirm that numa is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'iphone_se'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# database = Chroma(persist_directory=PERSIST_DIR, embedding_function=embedding_model)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# tools_idx = Chroma(collec)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m answer_list \u001b[38;5;241m=\u001b[39m answer_subqs(list_of_subqs)\n",
      "Cell \u001b[1;32mIn[13], line 26\u001b[0m, in \u001b[0;36manswer_subqs\u001b[1;34m(subq_list)\u001b[0m\n\u001b[0;32m     24\u001b[0m     loop \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mnew_event_loop()\n\u001b[0;32m     25\u001b[0m     asyncio\u001b[38;5;241m.\u001b[39mset_event_loop(loop)\n\u001b[1;32m---> 26\u001b[0m result \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(really_answer_subqs(subquestions\u001b[38;5;241m=\u001b[39msubq_list))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\site-packages\\nest_asyncio.py:98\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[1;34m(self, future)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[1;32mc:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\asyncio\\futures.py:203\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\Preetham\\miniconda3\\envs\\new-spec-env\\Lib\\asyncio\\tasks.py:314\u001b[0m, in \u001b[0;36mTask.__step_run_and_handle_result\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    312\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[0;32m    313\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[1;32m--> 314\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[1;32mIn[13], line 12\u001b[0m, in \u001b[0;36mreally_answer_subqs\u001b[1;34m(subquestions)\u001b[0m\n\u001b[0;32m     10\u001b[0m llm \u001b[38;5;241m=\u001b[39m MyLlama(model_path\u001b[38;5;241m=\u001b[39mMODEL_PATH)\u001b[38;5;241m.\u001b[39mget_llm()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subquestion \u001b[38;5;129;01min\u001b[39;00m subquestions:\n\u001b[1;32m---> 12\u001b[0m     db \u001b[38;5;241m=\u001b[39m collection_to_db[subquestion[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m     13\u001b[0m     async_tasks\u001b[38;5;241m.\u001b[39mappend(rag_it(subquestion[\u001b[38;5;241m0\u001b[39m], db, llm))\n\u001b[0;32m     14\u001b[0m answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39masync_tasks)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'iphone_se'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "#list of answers from answering subquestions\n",
    "answer_list = answer_subqs(list_of_subqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gathers all the answers and with some string processing, returns final answer\n",
    "unformatted_str = ''\n",
    "for answer in answer_list:\n",
    "    unformatted_str = unformatted_str + ' '+ answer\n",
    "print(\"Question: \", curr_question)\n",
    "print(\"Final Answer: \", unformatted_str.lstrip())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spec_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
